---
title: What I Read This Week.
description: Weekly Reading Digest. 
publishedAt: 2022-03-11
status: published
coverImage: "https://images.unsplash.com/photo-1495055154266-57bbdeada43e?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
---

# General deep dive into the LLM Model, AI technology that powers ChatGPT and related products.

## Finding raw data.
    - I my self used to wonder, how and where do they get all the data? There are generally two options:
      a. Crawl it yourself, like companies like ChatGPT does.
      b. You use a public repo of crawled webpages, like non-profit CommonCrawl.
      c. They are all this crawlers going around the internet. You start webpages and you follow all the links, and keep follwing the links and you keep indexing all the info. And endup having tons of data over time.
    - And this crowling goes through bunch of steps.
      a. filtering, webpages that you don't want data from like malware sites, markeating sites, adults sites, etc.
      b. filterin and processing only text, beside all the market language that is there.
      c. only crawling the thrugh those pages, which has english language or language that you wantor like sites that has english more then 65%.
      d. this is fourth
## Tokenization.
    - Raw data > bites > bytes > group frequent byte pairs into symbols = final's token.
    - cl100k_base is GPT-4 tokenizer.
## Nural Networks traning.

## Nural Networks Internals.
